Absolutely. Let’s go **far deeper**—we will build a **ABC-grade, enterprise-ready Teradata → Snowflake lift-and-shift blueprint** with granular technical, operational, and business-level details. I’ll break it into sections with explicit mapping, incompatibility handling, warehouse sizing, workload isolation, cutover, governance, and reporting.

---

# **Enterprise Teradata → Snowflake Lift & Shift Strategy – ABC Bank**

**Presenter:**  Chief GTM
**Audience:** Business stakeholders, Enterprise Data Architects, Data Engineers, Analytics Teams

---

## **1. Business Objectives & Value**

**Why migrate?**

* Teradata is costly to scale and constrained in concurrency.
* Snowflake enables **elastic compute/storage**, unlimited scaling, and **multi-persona workload isolation**.
* Supports regulatory compliance (FCA, GDPR) and audit requirements.
* Provides **modern BI & analytics integration**, including Tableau Cloud and PowerBI.

**Expected Benefits:**

1. **Cost Efficiency:** Auto-scale, auto-suspend warehouses; pay only for compute used.
2. **Infinite Concurrency:** Persona-based warehouses for ETL, BI, and ad-hoc users.
3. **Operational Simplicity:** Reduced dependency on hardware.
4. **Agility:** Faster new report creation, ad-hoc analytics, and data science capabilities.

---

## **2. Snowflake Application & Layered Architecture**

### **2.1 Database & Schema Mapping**

| Application | Snowflake Database | Snowflake Schema | Teradata Layer | Description                                  |
| ----------- | ------------------ | ---------------- | -------------- | -------------------------------------------- |
| FIN         | BASE_FIN           | E                | E              | Canonical, enterprise finance tables         |
|             |                    | L                | L              | Aggregated/derived finance datasets          |
|             | PREPARED_FIN       | A                | A              | Application-ready tables for finance reports |
|             |                    | U                | U              | Ad-hoc analytics, sandbox tables for users   |
| TRD         | BASE_TRD           | E                | E              | Trading source-of-truth data                 |
|             |                    | L                | L              | Derived/aggregated trading datasets          |
|             | PREPARED_TRD       | A                | A              | Reports-ready trading tables                 |
|             |                    | U                | U              | Ad-hoc analytics for traders                 |
| CUST        | BASE_CUST          | E                | E              | Customer canonical data                      |
|             |                    | L                | L              | Aggregated customer data                     |
|             | PREPARED_CUST      | A                | A              | Application-ready customer reports           |
|             |                    | U                | U              | Sandbox/ad-hoc analytics                     |

**Notes:**

* **BASE DB:** Holds source & logical layers (E + L) for auditing and lineage.
* **PREPARED DB:** Holds application/user layers (A + U) for analytics.
* **Schemas & Objects:** Uppercase enforced; special characters replaced by `_`.

---

### **2.2 Object Naming Conventions**

| Object Type | Convention                                    | Example           |
| ----------- | --------------------------------------------- | ----------------- |
| Database    | BASE_<App> / PREPARED_<App>                   | BASE_FIN          |
| Schema      | Uppercase: E / L / A / U                      | E                 |
| Table       | APP_LAYER_OBJECT                              | FIN_E_TXN         |
| View        | VW_<TABLE>                                    | VW_FIN_E_TXN      |
| Macro/UDTF  | UDTF_<MacroName>                              | UDTF_CALC_BALANCE |
| Columns     | Uppercase, max 255 chars, special chars → `_` | ACCOUNT_NO        |

**Rationale:** Ensures **case-insensitive compatibility**, avoids reserved keywords, and aligns with Snowflake best practices.

---

## **3. SQL, Macros, & Stored Logic Conversion**

| Teradata Component | Snowflake Strategy             | Notes                                                  |
| ------------------ | ------------------------------ | ------------------------------------------------------ |
| BTEQ scripts       | Convert to SnowSQL             | Maintain logic, replace Teradata-specific syntax       |
| Macros             | Convert to UDTFs or procedures | Complex macros may require JavaScript or SQL procedure |
| Stored Procedures  | Re-implemented                 | CTEs for recursion, loops in procedures                |
| QUALIFY / TOP      | ROW_NUMBER() OVER() with LIMIT | Snowflake equivalent                                   |
| Functions          | Map to Snowflake functions     | TO_DATE, TRY_CAST, COALESCE, etc.                      |
| Case sensitivity   | Uppercase enforced             | Prevent mismatches                                     |
| Reserved keywords  | Escape or rename               | Avoid errors                                           |

**Additional Considerations:**

* Recursive macros → recursive CTEs
* Error handling → Snowflake TRY_* functions
* Logging → Snowflake task history & query history

---

## **4. Data Migration Strategy**

### **4.1 Full Historical Load**

* **Extract:** Teradata → S3 / Cloud stage
* **Load:** Snowflake `COPY INTO`
* **Validation:** Row counts, checksums, data profiling

### **4.2 Incremental Load**

* **Method:** CDC or timestamp-based incremental loads
* **Parallel Run:** 2–3 weeks to validate consistency
* **Reconciliation:** Automated comparison of counts, aggregates, KPIs

### **4.3 Bridging Feeds**

* Optional feeds back to Teradata for downstream systems until cutover.
* Ensures **business continuity**.

---

## **5. Dedicated Warehouses & Workload Isolation**

| Persona | Warehouse   | Use Case                     | Configuration                           | Key Benefit                       |
| ------- | ----------- | ---------------------------- | --------------------------------------- | --------------------------------- |
| ETL     | ETL_<App>   | Batch & incremental loads    | Medium/Large, auto-suspend, auto-resume | No impact on BI or ad-hoc queries |
| BI      | BI_<App>    | Tableau / PowerBI dashboards | Multi-cluster, concurrency optimized    | Dashboards always performant      |
| Ad-hoc  | ADHOC_<App> | Analysts / Data Scientists   | Multi-cluster, auto-scale to N clusters | Handles unpredictable queries     |

**Notes:**

* Snowflake **compute is decoupled from storage**, enabling infinite scaling.
* Multi-cluster warehouses provide **high concurrency** without contention.
* Resource monitors enforce SLAs and budget constraints.

---

## **6. Teradata → Snowflake Compatibility Considerations**

| Issue                                   | Solution                                                         |
| --------------------------------------- | ---------------------------------------------------------------- |
| Data Types                              | Map numeric precision, VARCHAR length, LOB/BLOB → VARIANT/BINARY |
| Identity / Sequence columns             | Use Snowflake SEQUENCES                                          |
| Partitioning / Primary Keys             | Use clustering keys and micro-partitions                         |
| Functions                               | Map unsupported Teradata functions to Snowflake equivalents      |
| Column name length / special characters | Sanitize & truncate                                              |
| Macros / Stored Procs                   | UDTFs or Snowflake procedures                                    |
| Concurrency                             | Dedicated warehouses per persona                                 |

**Additional Checks:**

* Pre-migration scans for incompatible objects
* Automated naming and type normalization
* Logging of all conversion adjustments for audit

---

## **7. Reporting Migration Strategy**

| Source          | Target           | Strategy                                      |
| --------------- | ---------------- | --------------------------------------------- |
| Tableau On-Prem | Tableau Cloud    | Preserve dashboards, folders, and permissions |
| SAP BO          | PowerBI          | Recreate reports, map roles from Teradata     |
| Validation      | Cloud & Teradata | KPI reconciliation, row-level validation      |

**Goal:** Minimize **user adoption friction** by maintaining permission models.

---

## **8. Access Control & Governance**

* **Database-Level RBAC:** Map Teradata roles to Snowflake RBAC.
* **Schema-Level Access:** Fine-grained controls on A/U schemas.
* **Audit & Compliance:** Snowflake Access History, Object Activity, masked PII.
* **Provisioning:** Central request portal; approvals logged.
* **Encryption:** End-to-end, at rest and in transit.

---

## **9. Cutover & Parallel Run Plan**

| Phase                  | Activities                                           | Duration    |
| ---------------------- | ---------------------------------------------------- | ----------- |
| Assessment & Planning  | Inventory Teradata DBs, schema mapping, dependencies | Weeks 1–2   |
| Snowflake Setup        | Databases, warehouses, RBAC, governance              | Weeks 2–3   |
| Code Conversion        | BTEQ → SnowSQL, macros → UDTFs, stored procedures    | Weeks 3–6   |
| Historical Load        | Full migration & validation                          | Weeks 6–7   |
| Incremental / Parallel | CDC, reconciliation, bridging feeds                  | Weeks 7–10  |
| Reporting Migration    | Tableau Cloud & PowerBI                              | Weeks 9–11  |
| Cutover                | Switch all workloads to Snowflake                    | Weeks 11–12 |
| Post-Go-Live           | Monitoring, optimization, audit reconciliation       | Week 12+    |

---

## **10. Best Practices & Risk Mitigation**

* **Parallel Run:** Validate workloads before cutover.
* **Bridging Feeds:** Maintain downstream continuity.
* **Automated Reconciliation:** Catch mismatches early.
* **Governance & Audit:** Layered access, logging, encryption.
* **Performance Tuning:** Clustering keys, caching, multi-cluster warehouses.
* **Cost Optimization:** Auto-suspend/resume, scale warehouses per persona, resource monitors.

---

## **11. Expected Outcomes**

* **Cloud-Native Analytics:** Infinite scaling & concurrency.
* **Workload Isolation:** ETL, BI, and ad-hoc users do not impact each other.
* **Governed Data Model:** BASE/PREPARED layers aligned to Teradata lineage.
* **Smooth Reporting Migration:** Minimal disruption, consistent KPIs.
* **Compliance & Audit Ready:** Full logging, RBAC, and encrypted storage.

---

This document is **executive-ready** while providing **deep technical guidance** for Snowflake architects, ETL engineers, and BI teams.

---

If you want, I can **now create a detailed visual blueprint/diagram** showing:

* Teradata layers → Snowflake BASE/PREPARED → schemas
* Dedicated warehouses per persona
* ETL → Snowflake → BI → Ad-hoc flows
* Parallel run & bridging feed paths

This would be **boardroom-ready and ABC GTM style**.

Do you want me to create that visual next?














Perfect — let’s **break this down step by step** and go **far deeper**, including **constraints, considerations, risks, and mitigation strategies**. I’ll follow the full migration lifecycle from assessment to post-go-live.

---

# **Teradata → Snowflake Lift & Shift – Step-by-Step Detailed Blueprint**

---

## **Step 1: Assessment & Planning**

**Objective:** Understand the full Teradata landscape and define Snowflake target architecture.

**Activities:**

1. **Inventory Teradata Assets**

   * Databases, schemas, tables, views, macros, BTEQ scripts, stored procedures.
   * Identify **E, L, A, U layers**.
   * Analyze table sizes, row counts, data types, and growth patterns.

2. **Dependency Mapping**

   * Table-level dependencies (views, foreign keys, joins).
   * Cross-application dependencies.
   * Downstream systems consuming Teradata feeds.

3. **Workload Analysis**

   * Query frequency, runtime, concurrency patterns.
   * ETL job schedules, batch windows, critical reports.

4. **Security & Compliance Review**

   * Teradata roles and permissions.
   * Sensitive data, PII, GDPR/FCA compliance constraints.

**Constraints:**

* Large number of databases and schemas.
* Complex dependencies may require multiple passes to identify.
* Unknown downstream consumers.

**Considerations:**

* Use automated profiling tools for schema discovery and dependency analysis.
* Document all Teradata objects and dependencies in a **migration catalog**.

**Mitigation:**

* Build a **Teradata-to-Snowflake mapping matrix** early.
* Flag complex macros, recursive queries, or unsupported functions for early testing.

---

## **Step 2: Snowflake Setup & Architecture Design**

**Objective:** Create Snowflake environment aligned to enterprise and persona needs.

**Activities:**

1. **Database & Schema Creation**

   * Two databases per application: BASE & PREPARED.
   * Schemas: E, L, A, U.

2. **Warehouse Design**

   * Dedicated warehouses per application per persona:

     * ETL warehouse: Medium/Large
     * BI warehouse: Multi-cluster, concurrency optimized
     * Ad-hoc warehouse: Multi-cluster, auto-scale

3. **Naming Conventions**

   * Standardize for databases, schemas, tables, columns, views, procedures, and UDTFs.

4. **RBAC & Governance**

   * Map Teradata roles to Snowflake roles.
   * Schema-level fine-grained access for sensitive layers (A/U).
   * Enable auditing and logging.

**Constraints:**

* Snowflake name limits (DB: 255 chars, columns: 255 chars, case sensitivity).
* Multi-warehouse cost control.

**Considerations:**

* Define resource monitors per warehouse to prevent cost overrun.
* Uppercase naming to enforce case-insensitivity.

**Mitigation:**

* Build a **Snowflake architecture document** with warehouse sizing, persona assignments, and naming standards.
* Automate warehouse provisioning through IaC (Terraform or Snowflake scripts).

---

## **Step 3: Code Conversion (BTEQ → SnowSQL & Macros → UDTF)**

**Objective:** Convert Teradata scripts and logic to Snowflake-compatible code.

**Activities:**

1. **BTEQ Scripts**

   * Convert to SnowSQL scripts with parameterization.
   * Replace Teradata-specific commands (`.EXPORT`, `.LOGON`) with SnowSQL equivalents.

2. **Macros**

   * Rebuild as UDTFs or stored procedures.
   * Map recursion to CTEs.
   * Test macro output for row-level equivalence.

3. **Functions**

   * Map unsupported Teradata functions to Snowflake equivalents (`QUALIFY` → `ROW_NUMBER()`).
   * Handle string, date, numeric conversions.

4. **Error Handling**

   * Implement TRY_CAST or TRY_* equivalents to handle incompatible values.

**Constraints:**

* Complex macros or recursive logic may not have a direct equivalent.
* Case-sensitivity may break old scripts.

**Considerations:**

* Prioritize high-impact ETL processes first.
* Build a **code conversion library** for repeatable patterns.

**Mitigation:**

* Automate code scanning to detect incompatible constructs.
* Maintain parallel execution with validation for critical pipelines.

---

## **Step 4: Historical Data Load**

**Objective:** Load all existing Teradata data into Snowflake BASE database.

**Activities:**

1. **Data Extraction**

   * Teradata → S3 / Cloud stage → Snowflake staging tables.

2. **Data Load**

   * Use `COPY INTO` commands for bulk load.
   * Apply clustering keys for large tables.

3. **Validation**

   * Row counts, column-level checksums, key metrics, data profiling.

**Constraints:**

* Extremely large tables may hit Snowflake load limits per file (max 5TB per `COPY INTO` batch recommended).
* Transformation may be required during load for incompatible types.

**Considerations:**

* Stage data in compressed, partitioned formats (Parquet/CSV).
* Maintain incremental load columns for reconciliation.

**Mitigation:**

* Break very large tables into partitions during load.
* Log all load steps with error handling and retry mechanisms.

---

## **Step 5: Incremental & CDC Load**

**Objective:** Maintain Teradata → Snowflake sync until cutover.

**Activities:**

1. **CDC/Incremental Extraction**

   * Use timestamp columns or change tables.
   * Capture inserts, updates, deletes.

2. **Parallel Run**

   * Run Snowflake pipelines in parallel with Teradata.
   * Compare results to validate completeness and correctness.

3. **Reconciliation**

   * Automated scripts to compare row counts, aggregates, KPIs.
   * Flag mismatches for review.

**Constraints:**

* Change capture complexity varies by table.
* Parallel run requires extra compute and monitoring.

**Considerations:**

* Maintain consistent staging schema.
* Monitor lag between Teradata and Snowflake.

**Mitigation:**

* Introduce **bridging feeds** to allow downstream systems to consume Teradata until Snowflake is validated.

---

## **Step 6: Reporting Migration**

**Objective:** Move analytics workloads to cloud with minimal disruption.

**Activities:**

1. **Tableau On-Prem → Tableau Cloud**

   * Preserve dashboard structure and folder hierarchy.
   * Validate queries return identical metrics.

2. **SAP BO → PowerBI**

   * Recreate reports in PowerBI, map roles.
   * Validate KPIs and user filters.

3. **Validation**

   * Row-level and aggregate comparisons.
   * User acceptance testing (UAT) with business analysts.

**Constraints:**

* Some SAP BO logic may need reimplementation in PowerBI.
* Users may have embedded Teradata queries.

**Considerations:**

* Provide training sessions for Tableau Cloud / PowerBI.
* Retain existing permission models for low friction adoption.

**Mitigation:**

* Parallel reporting run for 2–3 weeks.
* Maintain a **report validation matrix** for reconciliation.

---

## **Step 7: User Access Replication & Governance**

**Objective:** Replicate Teradata RBAC and enforce schema-level controls in Snowflake.

**Activities:**

1. **Database-level Access**

   * Map Teradata roles → Snowflake roles.
   * Assign ETL / BI / Ad-hoc personas.

2. **Schema-level Fine-Grained Access**

   * E/L: read-only for most users.
   * A/U: write access for ETL or data scientists.

3. **Audit & Compliance**

   * Enable Access History, Object Activity logs.
   * Data masking for sensitive fields.

**Constraints:**

* Differences between Teradata roles and Snowflake RBAC may require custom mapping.
* Fine-grained access may introduce maintenance overhead.

**Considerations:**

* Automate provisioning via request portal.
* Document all access approvals for compliance.

**Mitigation:**

* Use Snowflake **role hierarchy** to simplify assignments.
* Periodic access review post-go-live.

---

## **Step 8: Cutover & Go-Live**

**Objective:** Fully switch workloads from Teradata to Snowflake.

**Activities:**

1. **Final Incremental Load**

   * Capture last changes from Teradata to Snowflake.
   * Validate reconciled data.

2. **Workload Switch**

   * Point ETL jobs, dashboards, and ad-hoc queries to Snowflake.

3. **Monitoring**

   * Real-time validation of critical reports.
   * Track warehouse performance and concurrency.

**Constraints:**

* Unexpected data discrepancies may occur.
* User adoption challenges.

**Considerations:**

* Communicate cutover plan to all stakeholders.
* Run critical dashboards in parallel for first few days.

**Mitigation:**

* Maintain bridging feeds for 1–2 weeks as fallback.
* Escalation matrix for production support.

---

## **Step 9: Post-Go-Live Optimization**

**Objective:** Ensure performance, governance, and cost efficiency.

**Activities:**

1. **Performance Tuning**

   * Adjust clustering keys and partitioning.
   * Monitor query times and concurrency.

2. **Cost Optimization**

   * Auto-suspend/resume warehouses.
   * Evaluate warehouse sizing for ETL, BI, ad-hoc.

3. **Governance**

   * Review role assignments.
   * Validate audit logs and masking rules.

4. **User Training & Support**

   * Provide documentation and runbooks.
   * Collect feedback for improvements.

**Constraints:**

* Cost may spike if warehouses auto-scale improperly.
* Users may continue to request Teradata access.

**Mitigation:**

* Implement **resource monitors** and alerts.
* Disable Teradata access progressively after Snowflake stabilization.

---

## **Step 10: Risk & Mitigation Summary**

| Risk                  | Mitigation                                              |
| --------------------- | ------------------------------------------------------- |
| Data incompatibility  | Automated scanning, UDTF conversion, reconciliation     |
| Reporting mismatch    | Parallel report validation, UAT                         |
| Concurrency conflicts | Dedicated warehouses per persona, multi-cluster scaling |
| Cost overrun          | Auto-suspend/resume, resource monitors                  |
| User adoption         | Training, preserve permissions, phased migration        |
| Downstream disruption | Bridging feeds, parallel run, final reconciliation      |

---

This **step-by-step blueprint** now includes **detailed constraints, considerations, and mitigations for each stage**, making it **actionable for ABC or similar enterprise banks**.

---

If you want, I can **also create a master visual diagram** showing:

* Teradata → Snowflake mapping per application & layer
* Warehouses per persona
* Data pipelines & reporting flows
* Cutover & parallel run paths

This would be **boardroom-ready and highly detailed**.

Do you want me to create that next?






















Perfect — now we can incorporate **non-production and development environments, logical separation, and security governance** into the strategy. Let’s expand the blueprint with **environment strategy, security considerations, and governance forums**.

---

# **Enhanced Teradata → Snowflake Lift & Shift Strategy: Environment & Security Governance**

---

## **1. Environment Strategy**

**Objective:** Define where development, testing, and production workloads will reside while maintaining security, isolation, and compliance.

### **1.1 Environment Design**

| Environment              | Purpose                                                        | Databases / Schemas                                                | Access / Considerations                                                                              |
| ------------------------ | -------------------------------------------------------------- | ------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- |
| **Production (PROD)**    | Live data, reporting, ETL jobs                                 | BASE & PREPARED databases for all applications                     | Restricted access; monitored by governance; read/write per RBAC policies                             |
| **Development (DEV)**    | ETL / BI development, macro/procedure creation, script testing | Logical non-prod databases mirroring PROD (BASE_DEV, PREPARED_DEV) | No live business data; can contain synthetic or anonymized data                                      |
| **Testing / UAT (TEST)** | Validation of ETL, incremental load, and reporting             | BASE_TEST, PREPARED_TEST                                           | Controlled access; mirrors PROD schema but smaller sample of data; used for parallel run simulations |
| **Staging (STAGE)**      | Pre-production testing, final validation before cutover        | BASE_STAGE, PREPARED_STAGE                                         | Only for reconciliation; bridges Teradata → Snowflake final validation                               |

**Notes:**

* **All development and testing happen in Snowflake PROD account** with **logical separation** via dedicated databases (`_DEV`, `_TEST`, `_STAGE`).
* This ensures **security compliance**, consistent governance, and avoids the overhead of maintaining multiple Snowflake accounts.

### **1.2 Data Strategy in Non-Prod**

* **Synthetic / Masked Data:**

  * DEV and TEST databases should use masked or synthetic data where PII exists.
* **Incremental Loads:**

  * Parallel incremental updates can be simulated with a subset of production data in TEST / STAGE environments.
* **Code Promotion:**

  * ETL scripts, UDTFs, SnowSQL, and stored procedures are developed in DEV → tested in TEST → promoted to STAGE → PROD.

**Constraints:**

* Developers cannot access sensitive production data directly.
* Non-prod databases must remain logically isolated to prevent accidental updates.

**Mitigation:**

* Use **Snowflake roles and RBAC** for environment separation: `DEV_ROLE`, `TEST_ROLE`, `PROD_ROLE`.
* Track all code promotions using **Git / CI/CD pipelines** integrated with Snowflake.

---

## **2. Security Governance & Forums**

**Objective:** Ensure enterprise-wide agreement on migration approach, environment setup, and access controls.

### **2.1 Security & Compliance Considerations**

| Area               | Approach / Control                        | Notes                                                                                 |
| ------------------ | ----------------------------------------- | ------------------------------------------------------------------------------------- |
| Access Control     | Role-based RBAC per environment           | DEV/TEST have restricted synthetic data access; PROD has strict approval-based access |
| Data Masking       | Mask PII in DEV/TEST                      | Use Snowflake Dynamic Data Masking for sensitive columns                              |
| Encryption         | Always-on encryption                      | Snowflake-managed keys or BYOK (Bring Your Own Key) per enterprise policy             |
| Audit & Monitoring | Snowflake Access History, Object Activity | Audit logs retained for required regulatory period                                    |
| Network Security   | Private endpoints, VPC / PrivateLink      | Avoid public exposure of Snowflake account                                            |
| Change Approval    | Security / Architecture Forum             | All environment designs, roles, and cutover approaches reviewed and approved          |

---

### **2.2 Security & Architecture Forum**

* **Purpose:** Align stakeholders on environment design, data access, and governance standards.
* **Participants:**

  * Enterprise Security / Information Security Team
  * Data Governance / Compliance Officers
  * Data Architects & ETL Leads
  * Application Owners / Business SMEs
* **Responsibilities:**

  * Approve DEV / TEST / STAGE setup in PROD account.
  * Sign-off on masked or synthetic data for non-prod environments.
  * Validate role-based access and privilege escalation controls.
  * Confirm logging, audit, and monitoring compliance.

---

### **2.3 Mitigation of Security Risks**

| Risk                               | Mitigation                                                                |
| ---------------------------------- | ------------------------------------------------------------------------- |
| Accidental access to PROD from DEV | Logical isolation via separate databases, strict RBAC, approval workflows |
| PII leakage in TEST / DEV          | Masked/synthetic data enforced via Snowflake masking policies             |
| Uncontrolled code promotion        | CI/CD pipeline with approval gates; code repository versioning            |
| Non-compliance with audit          | Enable all Snowflake audit logs; periodic review by governance forum      |

---

## **3. Integration into Migration Lifecycle**

1. **Assessment & Planning**

   * Security forum confirms non-prod environments and masked data policy.
2. **Snowflake Setup**

   * Create DEV / TEST / STAGE databases within PROD account.
   * Configure warehouses for each persona in DEV/TEST for testing ETL and BI workloads.
3. **Code Conversion & Testing**

   * Develop BTEQ → SnowSQL, macros → UDTF in DEV.
   * Validate in TEST with subset/masked data.
4. **Historical & Incremental Load**

   * Load non-sensitive test data in DEV / TEST for validation.
   * Confirm reconciliation before moving to PROD.
5. **Reporting Migration**

   * BI dashboards tested in DEV/TEST first.
   * Security forum confirms access and masking before production cutover.
6. **Cutover & Go-Live**

   * Execute PROD migration only after DEV/TEST validation and forum sign-off.

---

### **4. Additional Considerations**

* **Cost Management:** Non-prod databases (DEV / TEST) can have smaller warehouses; auto-suspend and auto-resume enforced.
* **Performance Testing:** TEST / STAGE environments replicate production schemas to simulate ETL and reporting workloads.
* **Change Control:** Every environment must follow **formal change approval** before promotion to production.

---

✅ **Key Takeaways:**

* Development and testing **happen logically within the PROD Snowflake account**.
* DEV/TEST databases mirror PROD schemas but use **synthetic/masked data**.
* **Security and architecture forum** is mandatory to approve environment setup, access controls, and data masking policies.
* Environment separation + governance ensures compliance, risk mitigation, and smooth cutover.

---

If you want, I can **update the full step-by-step migration blueprint to include these environment and security layers**, showing **DEV / TEST / STAGE integration at each step**, **warehouse allocation**, and **environment-specific access controls** — making it **a complete ABC-ready migration playbook**.

Do you want me to do that next?












Perfect — now we can incorporate **Snowflake cloning** for non-prod environments, which is a **key Snowflake best practice** for development, testing, and continuous integration while saving storage costs. Let’s integrate this into the strategy.

---

# **Snowflake Non-Prod Strategy with Cloning**

---

## **1. Objective**

Enable **DEV and TEST environments** to:

* Have **up-to-date copies of PROD data** for development and testing.
* Save storage costs via **Snowflake zero-copy cloning**.
* Support **continuous development and testing** without impacting PROD workloads.

---

## **2. Environment Cloning Approach**

| Environment      | Cloning Strategy                                                              | Benefits                                                                                         |
| ---------------- | ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |
| DEV              | Clone PROD BASE & PREPARED databases → `BASE_DEV_<App>`, `PREPARED_DEV_<App>` | Up-to-date data for development without using additional storage; supports iterative development |
| TEST / UAT       | Clone DEV or PROD databases → `BASE_TEST_<App>`, `PREPARED_TEST_<App>`        | Safe testing environment; smaller warehouses; masked data optional                               |
| STAGE / Pre-prod | Clone PROD databases → `BASE_STAGE_<App>`, `PREPARED_STAGE_<App>`             | Validate final ETL, reports, and incremental loads before cutover                                |

**Notes:**

* Snowflake **zero-copy cloning** creates a **logical copy without duplicating storage**, only storing changes.
* Clones can be refreshed periodically (daily, weekly) to align with PROD.

---

## **3. Data Masking & Security in Clones**

* DEV/TEST clones can apply **masking policies** to sensitive columns (PII, financial data).
* Masked or synthetic data ensures compliance while allowing developers to work with realistic datasets.
* Access controls for clones are managed via **Snowflake RBAC roles** (`DEV_ROLE`, `TEST_ROLE`) distinct from PROD roles.

---

## **4. Continuous Development / CI/CD**

* **Workflow:**

  1. Clone PROD → DEV environment.
  2. Developers implement ETL, SnowSQL scripts, UDTFs, procedures.
  3. Run automated tests in DEV using cloned data.
  4. Promote to TEST / UAT clone for validation.
  5. Once validated, promote to STAGE / PROD using approved CI/CD pipelines.

* **Advantages:**

  * Reduces storage cost (zero-copy cloning).
  * Ensures DEV/TEST are aligned with current PROD state.
  * Enables **iterative development without downtime**.

---

## **5. Refresh & Retention Policies**

| Clone Environment | Refresh Frequency    | Retention / Considerations                          |
| ----------------- | -------------------- | --------------------------------------------------- |
| DEV               | Daily / Weekly       | Keep only latest snapshot; old clones auto-drop     |
| TEST              | Weekly / On-demand   | Align with parallel run cycles; mask sensitive data |
| STAGE             | Before major cutover | Reflect final PROD state for final validation       |

**Mitigation:**

* Automate cloning & masking via Snowflake tasks or orchestration tools (Airflow, dbt, etc.).
* Monitor storage usage; cloning reduces physical storage but modifications are billed.

---

## **6. Constraints & Considerations**

* **Constraint:** Cloning large databases may temporarily increase metadata management overhead.
* **Constraint:** Continuous development on cloned databases may diverge from PROD; regular refresh is necessary.
* **Consideration:** Apply masking policies on clones before providing developer access.
* **Consideration:** Ensure clones do not interfere with PROD warehouses; assign separate warehouses for DEV/TEST workloads.

---

## **7. Security & Governance**

* All clones must be reviewed and approved by **Security & Architecture Forum**.
* RBAC ensures DEV/TEST developers cannot inadvertently access PROD directly.
* Logging and auditing still tracked in Snowflake; all queries on clones are visible for compliance.

---

✅ **Key Takeaways:**

* Non-prod environments (DEV / TEST / STAGE) **are created via zero-copy cloning** from PROD databases.
* **Masked or synthetic data** ensures compliance while maintaining realistic datasets.
* **Continuous development pipelines** leverage these clones for ETL, UDTFs, and report validation.
* **Cost-efficient, secure, and aligned with PROD** for iterative development and testing.

---

I can now **update the full step-by-step migration blueprint**, integrating **DEV / TEST / STAGE with cloning**, warehouse assignments, masking policies, and cutover planning — making it a **complete enterprise-ready ABC migration playbook**.

Do you want me to produce that fully integrated blueprint next?




















Absolutely — here’s a **fully consolidated, enterprise-grade Teradata → Snowflake lift-and-shift strategy** integrating all aspects: layered mapping, non-prod environments with cloning, security, governance, workloads, reporting, cutover, and mitigations. I’ve structured it for **ABC-level clarity**, combining business and technical detail.

---

# **Enterprise Teradata → Snowflake Lift & Shift Strategy**

**Audience:** Business stakeholders, Enterprise Architects, Data Engineers, Analytics Teams
**Presenter:** f GTM

---

## **1. Executive Summary**

* Modernize Teradata workloads to Snowflake: elastic compute/storage, high concurrency, cloud-native reporting.
* Ensure **business continuity** via parallel run, bridging feeds, and incremental replication.
* Enable **persona-based warehouses** for ETL, BI, and ad-hoc queries.
* Implement **secure, governed environments** for development, testing, and production.
* Migrate BI/reporting tools: Tableau On-Prem → Tableau Cloud, SAP BO → PowerBI, retaining permissions for user adoption.

---

## **2. Snowflake Architecture & Database Mapping**

### **2.1 Application & Layered Design**

| Application | Snowflake Database | Snowflake Schema | Teradata Layer | Purpose                     |
| ----------- | ------------------ | ---------------- | -------------- | --------------------------- |
| FIN         | BASE_FIN           | E                | E              | Canonical finance tables    |
|             |                    | L                | L              | Aggregated finance datasets |
|             | PREPARED_FIN       | A                | A              | Application-ready tables    |
|             |                    | U                | U              | Ad-hoc analytics            |
| TRD         | BASE_TRD           | E                | E              | Trading source-of-truth     |
|             |                    | L                | L              | Derived trading datasets    |
|             | PREPARED_TRD       | A                | A              | Reports-ready tables        |
|             |                    | U                | U              | Sandbox/ad-hoc analytics    |
| CUST        | BASE_CUST          | E                | E              | Customer canonical data     |
|             |                    | L                | L              | Aggregated customer data    |
|             | PREPARED_CUST      | A                | A              | Application-ready reports   |
|             |                    | U                | U              | Sandbox/ad-hoc analytics    |

**Key Principles:**

* BASE: E + L layers for lineage & auditing.
* PREPARED: A + U layers for analytics consumption.
* Uppercase naming for tables, columns, views, procedures, and UDTFs.

---

### **2.2 Naming Conventions**

| Object Type    | Convention                  | Example           |
| -------------- | --------------------------- | ----------------- |
| Database       | BASE_<App> / PREPARED_<App> | BASE_FIN          |
| Schema         | Uppercase                   | E                 |
| Table          | APP_LAYER_OBJECT            | FIN_E_TXN         |
| View           | VW_<TABLE>                  | VW_FIN_E_TXN      |
| Procedure/UDTF | UDTF_<MacroName>            | UDTF_CALC_BALANCE |
| Column         | Uppercase, max 255 chars    | ACCOUNT_NO        |

**Rationale:** Enforces **case-insensitivity**, avoids reserved keywords, ensures enterprise standardization.

---

## **3. Non-Prod Environment Strategy with Cloning**

| Environment | Databases                              | Source / Cloning          | Purpose                                         |
| ----------- | -------------------------------------- | ------------------------- | ----------------------------------------------- |
| DEV         | BASE_DEV_<App>, PREPARED_DEV_<App>     | Zero-copy clone from PROD | ETL / BI development; continuous development    |
| TEST/UAT    | BASE_TEST_<App>, PREPARED_TEST_<App>   | Clone DEV or PROD         | Validation of ETL, incremental loads, reporting |
| STAGE       | BASE_STAGE_<App>, PREPARED_STAGE_<App> | Clone PROD                | Pre-production validation, final reconciliation |

**Key Points:**

* Zero-copy cloning saves storage while providing **up-to-date datasets**.
* DEV / TEST may apply **data masking** for PII / sensitive columns.
* Warehouses in DEV/TEST are smaller and auto-suspend to optimize costs.

---

## **4. Code & SQL Conversion**

| Teradata Component | Snowflake Approach            |
| ------------------ | ----------------------------- |
| BTEQ scripts       | SnowSQL scripts               |
| Macros             | UDTFs or procedures           |
| Stored Procedures  | Rebuilt in Snowflake (SQL/JS) |
| Recursive queries  | CTEs                          |
| Functions          | Map to Snowflake equivalents  |
| Case sensitivity   | Uppercase enforced            |
| Reserved keywords  | Escaped or renamed            |

**Considerations:**

* Automate code scanning for incompatibilities.
* Parallel run for validation.
* Maintain versioning in Git for CI/CD promotion from DEV → TEST → STAGE → PROD.

---

## **5. Data Migration Strategy**

### **5.1 Historical Load**

* Extract: Teradata → S3 / stage → Snowflake.
* Load: `COPY INTO` into BASE databases.
* Validation: row counts, checksums, KPI comparison.

### **5.2 Incremental / CDC**

* Timestamp or change-based incremental loads.
* Parallel run for 2–3 weeks.
* Automated reconciliation scripts.

### **5.3 Bridging Feeds**

* Optional feeds back to Teradata for downstream systems during parallel run.

**Constraints & Mitigations:**

* Large tables → partitioned loads.
* Incompatible types → transformations during load.
* Logging all load errors for audit and troubleshooting.

---

## **6. Warehouses & Workload Isolation**

| Persona | Warehouse   | Purpose                      | Configuration                        |
| ------- | ----------- | ---------------------------- | ------------------------------------ |
| ETL     | ETL_<App>   | Batch & incremental loads    | Medium/Large, auto-suspend/resume    |
| BI      | BI_<App>    | Tableau / PowerBI dashboards | Multi-cluster, concurrency optimized |
| Ad-hoc  | ADHOC_<App> | Analysts / data scientists   | Multi-cluster, auto-scale            |

**Notes:**

* Infinite scalability & concurrency.
* Isolated workloads prevent BI or ad-hoc queries from impacting ETL.
* Resource monitors enforce cost and performance limits.

---

## **7. Reporting Migration**

| Source          | Target           | Strategy                                        |
| --------------- | ---------------- | ----------------------------------------------- |
| Tableau On-Prem | Tableau Cloud    | Preserve dashboards, hierarchy, and permissions |
| SAP BO          | PowerBI          | Recreate reports, map roles                     |
| Validation      | Cloud & Teradata | KPI reconciliation, row-level checks            |

**Considerations:**

* Maintain existing permission model to reduce adoption friction.
* Run parallel reporting in TEST environment prior to cutover.

---

## **8. User Access & Security Governance**

* Database-level RBAC: map Teradata roles → Snowflake roles.
* Schema-level fine-grained access: A/U schemas for development, ETL access.
* Audit & Compliance: Snowflake Access History, Object Activity logs.
* Encryption: end-to-end, BYOK or Snowflake-managed keys.
* Masking policies applied on DEV/TEST clones for PII.

**Governance Forum:**

* Security & Architecture Forum reviews environment strategy, masking policies, and RBAC assignments.
* Required approvals before DEV/TEST clones or PROD access.

---

## **9. Cutover Plan & Parallel Run**

| Phase                  | Activities                                           | Duration    |
| ---------------------- | ---------------------------------------------------- | ----------- |
| Assessment & Planning  | Inventory, mapping, dependencies, security approvals | Weeks 1–2   |
| Snowflake Setup        | Databases, warehouses, RBAC, governance              | Weeks 2–3   |
| Code Conversion        | BTEQ → SnowSQL, macros → UDTFs                       | Weeks 3–6   |
| Historical Load        | Full migration, validation                           | Weeks 6–7   |
| Incremental / Parallel | CDC, reconciliation, bridging feeds                  | Weeks 7–10  |
| Reporting Migration    | Tableau Cloud & PowerBI                              | Weeks 9–11  |
| Cutover                | Switch workloads to Snowflake                        | Weeks 11–12 |
| Post-Go-Live           | Monitoring, optimization, audit                      | Week 12+    |

**Mitigations:**

* Bridging feeds to Teradata for critical systems.
* Staggered reporting validation to avoid disruption.
* Parallel run ensures reconciliation before full cutover.

---

## **10. Risk, Constraints & Mitigation**

| Risk / Constraint     | Mitigation                                                               |
| --------------------- | ------------------------------------------------------------------------ |
| Data incompatibility  | Automated scanning, conversion libraries, reconciliation                 |
| Large table loads     | Partitioned loads, staged extraction, parallel processing                |
| Security / PII        | Masking policies on clones, role-based access, governance forum approval |
| Concurrency conflicts | Dedicated warehouses per persona, multi-cluster scaling                  |
| Cost overrun          | Auto-suspend/resume, resource monitors, warehouse sizing optimization    |
| User adoption         | Training, preserve permissions, staged reporting migration               |

---

## **11. Best Practices**

* DEV / TEST / STAGE environments **logically separated within PROD account**.
* DEV / TEST use **clones** of PROD for storage efficiency and realistic testing.
* CI/CD pipeline manages promotion of scripts, UDTFs, and procedures.
* Parallel run and bridging feeds ensure **business continuity**.
* Dedicated warehouses per persona prevent workload interference.
* Security & architecture forum validates all environment, access, and masking decisions.

---

## **12. Expected Outcomes**

* Cloud-native, infinitely scalable analytics platform.
* Persona-based warehouse isolation ensures high concurrency and performance.
* BASE / PREPARED layer model aligned with Teradata lineage.
* Smooth reporting migration, minimal disruption, and user adoption.
* Governance, compliance, and audit-ready environment.

---

This is now a **fully consolidated, detailed, enterprise-ready Teradata → Snowflake migration strategy** including:

* Layered database mapping
* Non-prod cloning strategy
* Security, masking, and governance
* Workload and warehouse allocation
* Reporting migration plan
* Cutover, parallel run, risk mitigations, and CI/CD promotion

---

If you want, I can **also produce a visual, end-to-end diagram** showing:

1. Teradata → Snowflake database & layer mapping
2. DEV/TEST/STAGE clones from PROD
3. Warehouse/persona allocation
4. ETL & reporting flows
5. Parallel run & cutover stages

This would be **ABC boardroom-ready**.

Do you want me to generate that visual next?





Got it — you want a **fully consolidated, step-by-step Teradata → Snowflake lift-and-shift approach**, where **every step explicitly includes constraints, considerations, and mitigations**, plus **non-prod cloning, environment strategy, governance, warehouses, and reporting migration**. Here's the **complete approach**:

---

# **Enterprise Teradata → Snowflake Lift & Shift Strategy**

**Audience:** Business stakeholders, Data Architects, Data Engineers
**Presenter:** f GTM

---

## **1. Assessment & Planning**

**Objective:** Inventory Teradata assets, dependencies, and define target Snowflake architecture.

**Activities:**

* Inventory databases, tables, views, macros, BTEQ scripts, stored procedures.
* Map Teradata layers: E/L → BASE, A/U → PREPARED.
* Analyze workloads: batch ETL schedules, concurrency patterns, critical reports.
* Identify downstream dependencies, reporting feeds, and regulatory constraints.

**Constraints:**

* Large number of objects, complex dependencies.
* Unknown downstream consumers.
* Legacy macros/stored procedures may lack direct Snowflake equivalents.

**Considerations:**

* Automated scanning for schema, objects, and dependencies.
* Early identification of complex macros for testing.
* Document data growth trends for warehouse sizing.

**Mitigation:**

* Maintain a Teradata → Snowflake **mapping catalog**.
* Engage business owners to confirm downstream consumers.
* Flag high-risk objects for early proof-of-concept conversion.

---

## **2. Snowflake Environment Strategy**

**Objective:** Define PROD, DEV, TEST, STAGE, warehouse allocation, and access control.

**Activities:**

* **Production (PROD):** BASE & PREPARED databases per application.
* **Non-Prod Environments:** DEV, TEST, STAGE created via **zero-copy clones** from PROD to save storage and allow continuous development.
* Assign **dedicated warehouses per persona**: ETL, BI dashboards, Ad-hoc queries.
* Define **naming conventions** and RBAC roles.

**Constraints:**

* Snowflake object limits (DB/table/column ≤ 255 chars).
* Storage usage for modified clones.
* Warehouse sizing affects concurrency and costs.

**Considerations:**

* Clones allow DEV/TEST to use up-to-date data without extra storage.
* Mask PII and sensitive data in non-prod environments.
* Auto-suspend/resume warehouses to optimize costs.

**Mitigation:**

* Automate clone refresh cycles (daily/weekly).
* Separate warehouses for DEV/TEST to avoid impacting PROD workloads.
* Review architecture with Security & Architecture Forum before granting access.

---

## **3. Database & Layer Mapping**

| Application | Snowflake Database | Snowflake Schema | Teradata Layer | Purpose                     |
| ----------- | ------------------ | ---------------- | -------------- | --------------------------- |
| FIN         | BASE_FIN           | E                | E              | Canonical finance tables    |
|             |                    | L                | L              | Aggregated finance datasets |
|             | PREPARED_FIN       | A                | A              | Application-ready tables    |
|             |                    | U                | U              | Ad-hoc analytics            |
| TRD         | BASE_TRD           | E                | E              | Trading source-of-truth     |
|             |                    | L                | L              | Derived trading datasets    |
|             | PREPARED_TRD       | A                | A              | Reports-ready tables        |
|             |                    | U                | U              | Sandbox/ad-hoc analytics    |
| CUST        | BASE_CUST          | E                | E              | Customer canonical data     |
|             |                    | L                | L              | Aggregated customer data    |
|             | PREPARED_CUST      | A                | A              | Application-ready reports   |
|             |                    | U                | U              | Sandbox/ad-hoc analytics    |

**Constraints:**

* Schema and table names must adhere to Snowflake character limits.
* Conflicting object names across applications.

**Mitigation:**

* Standardized naming conventions and enforced uppercase.
* Prefixes per application (e.g., FIN_, TRD_, CUST_).

---

## **4. Code Conversion (BTEQ → SnowSQL, Macros → UDTFs/Procedures)**

**Activities:**

* Convert BTEQ scripts to SnowSQL.
* Convert macros to UDTFs or procedures.
* Map Teradata functions to Snowflake equivalents (QUALIFY → ROW_NUMBER()).
* Handle recursive queries using CTEs.
* Enforce uppercase naming and escape reserved keywords.

**Constraints:**

* Some macros or recursive logic may not have direct Snowflake equivalents.
* Case sensitivity differences may break legacy scripts.

**Considerations:**

* Use DEV clones to test conversions.
* Maintain versioning and CI/CD pipelines.

**Mitigation:**

* Automated code scanning for incompatibilities.
* Parallel run against Teradata outputs for validation.
* CI/CD promotion with approvals.

---

## **5. Data Migration Strategy**

**5.1 Historical Load**

* Extract Teradata → S3 → Snowflake staging → BASE database.
* Validation: row counts, checksums, KPI comparison.

**5.2 Incremental/CDC Load**

* Timestamp or change-based extraction.
* Parallel run to Teradata for 2–3 weeks.
* Reconciliation automated with alerts.

**Constraints:**

* Large tables may need partitioned loads.
* Incompatible types.

**Considerations:**

* Stage data in compressed, partitioned format (Parquet).
* Track all ETL steps for audit.

**Mitigation:**

* Split large tables into batches.
* Transform incompatible types during load.
* Log errors and enable retries.

**5.3 Bridging Feeds**

* Optional feeds back to Teradata to support downstream systems until Snowflake validated.

---

## **6. Warehouses & Workload Isolation**

| Persona | Warehouse   | Constraint                               | Mitigation                               |
| ------- | ----------- | ---------------------------------------- | ---------------------------------------- |
| ETL     | ETL_<App>   | Large batch jobs may block BI            | Dedicated warehouse, auto-suspend/resume |
| BI      | BI_<App>    | High concurrency may exceed cluster size | Multi-cluster, auto-scale                |
| Ad-hoc  | ADHOC_<App> | Unpredictable queries can spike cost     | Multi-cluster, resource monitors         |

**Considerations:**

* Infinite scalability due to Snowflake decoupled storage/compute.
* Separate warehouses per persona to prevent workload interference.

---

## **7. Reporting Migration**

| Source          | Target        | Constraints                             | Mitigation                                       |
| --------------- | ------------- | --------------------------------------- | ------------------------------------------------ |
| Tableau On-Prem | Tableau Cloud | Embedded Teradata queries               | Repoint to Snowflake, validate KPIs              |
| SAP BO          | PowerBI       | Some logic may require reimplementation | UAT with business stakeholders, phased migration |

**Considerations:**

* Preserve dashboard structure and permissions.
* Run reports in TEST environment before PROD cutover.

---

## **8. User Access & Security Governance**

* Database-level RBAC: map Teradata roles → Snowflake roles.
* Schema-level fine-grained access: A/U layers for ETL/scientists.
* Mask PII on DEV/TEST clones.
* Audit & compliance: Snowflake Access History & Object Activity logs.
* Encryption: end-to-end; BYOK optional.

**Constraints:**

* Differences in Teradata → Snowflake RBAC.
* Non-prod environments must not expose sensitive data.

**Mitigation:**

* Security & Architecture Forum approves access setup and masking.
* Periodic access review post-go-live.
* CI/CD controlled promotion of roles and privileges.

---

## **9. Cutover & Parallel Run**

* Final incremental load and reconciliation.
* Switch ETL, BI, ad-hoc queries to Snowflake.
* Monitor performance, concurrency, and KPIs.

**Constraints:**

* Unexpected data discrepancies.
* User adoption friction.

**Mitigation:**

* Maintain bridging feeds to Teradata during validation.
* Escalation matrix for support.
* Staggered reporting cutover to reduce disruption.

---

## **10. Risk, Constraints & Mitigation Summary**

| Risk / Constraint     | Mitigation                                                            |
| --------------------- | --------------------------------------------------------------------- |
| Data incompatibility  | Automated scanning, conversion library, reconciliation                |
| Large table loads     | Partitioned loads, parallel processing, staged extraction             |
| Security / PII        | Masking policies on clones, RBAC enforcement, forum approval          |
| Concurrency conflicts | Dedicated warehouses per persona, multi-cluster scaling               |
| Cost overrun          | Auto-suspend/resume, resource monitors, warehouse sizing optimization |
| User adoption         | Training, preserve permissions, phased reporting migration            |
| Non-prod divergence   | Regular clone refresh from PROD, masking applied                      |
| Governance gaps       | Security forum approvals, audit logging                               |

---

## **11. Best Practices**

* DEV/TEST/STAGE **reside logically in PROD account**.
* Use **zero-copy clones** for non-prod to save storage and enable continuous development.
* Persona-based warehouses prevent workload interference.
* Parallel run ensures reconciliation before cutover.
* Security & Architecture forum approves environments, access, masking.
* CI/CD pipeline governs code promotion.

---

This **consolidated approach** now fully integrates:

* Database/layer mapping
* DEV/TEST/STAGE cloning
* Warehouses per persona
* Code conversion
* Data migration strategy
* Reporting migration
* User access & governance
* Cutover, parallel run, risk mitigation

It is **enterprise-ready, business-friendly, and technically detailed**.

---

If you want, I can **also create a one-page visual architecture diagram** showing:

* Teradata layers → Snowflake BASE/PREPARED
* DEV/TEST/STAGE clones
* Warehouse/persona mapping
* ETL & reporting flows
* Parallel run / cutover paths

This would make it **fully boardroom-ready**.

Do you want me to create that diagram next?

